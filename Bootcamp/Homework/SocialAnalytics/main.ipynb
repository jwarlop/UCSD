{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Mood\n",
    "## John M Warlop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = {'BBC':'@BBC', 'CBS':'@CBS', 'NYTimes':'@nytimes', 'Fox':'@FoxNews', 'CNN':'@CNN'}\n",
    "type_handles = ['compound','neu','neg','pos','dtime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys():\n",
    "    s_fname = 'api_keys'\n",
    "    api_keys = {}\n",
    "    with open(s_fname) as f:\n",
    "        api_keys = json.load(f)\n",
    "    f.close()\n",
    "    return(api_keys)\n",
    "\n",
    "def pickle_it(data,fname):\n",
    "    import pickle\n",
    "    pckl_out = open(fname,\"wb\")\n",
    "    pickle.dump(data,pckl_out)\n",
    "    pckl_out.close()\n",
    "\n",
    "def logging_(log_name):\n",
    "    import logging\n",
    "    LOG_FORMAT = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "    logging.basicConfig(filename=log_name,level=logging.INFO,format=LOG_FORMAT,filemode='w')\n",
    "    return(logging.getLogger())\n",
    "\n",
    "def twitter_handshake(api_keys):\n",
    "    auth = tweepy.OAuthHandler(api_keys['consumer_key'],api_keys['consumer_secret_key'])\n",
    "    auth.set_access_token(api_keys['access_token'],api_keys['access_token_secret'])\n",
    "    api = tweepy.API(auth,parser=tweepy.parsers.JSONParser())\n",
    "    return(api)\n",
    "\n",
    "def sentiment_analysis(tweet):\n",
    "    # Import and Initialize Sentiment Analyzer\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    #results is dict{'compound':w,'neg':x,'neu':y,''pos':z} where w,x,y and z are floats\n",
    "    return(analyzer.polarity_scores(tweet)) \n",
    "\n",
    "def return_data_vector(data,name,type_):\n",
    "    the_list = data[name]\n",
    "    r_vector = []\n",
    "    for idx, cell in enumerate(the_list):\n",
    "        r_vector.append(cell[type_])\n",
    "    return(r_vector)\n",
    "\n",
    "def combine_vectors(data):\n",
    "    all_vectors = {}\n",
    "    for key,val in handles.items():\n",
    "        all_vectors[key]={}\n",
    "        for sentiment_param in type_handles:\n",
    "            all_vectors[key][sentiment_param]=return_data_vector(data,key,sentiment_param)\n",
    "    return(all_vectors)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "logger = logging_(\"main.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API Keys\n",
    "logger.info(\"Loading API keys\")\n",
    "api_keys = get_keys()\n",
    "logger.info(\"Keys loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweepy/Twitter Handshake\n",
    "logger.info(\"Sending keys to twitter\")\n",
    "api = twitter_handshake(api_keys)\n",
    "logger.info(\"Keys accepted by twitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_200_tweets = {}\n",
    "for idx, key in enumerate(handles): #Loop through spewers of fake news('CNN','FOX', etc)\n",
    "    public_tweets = api.user_timeline(handles[key],count=200,tweet_mode=\"extended\")\n",
    "    last_200_tweets[key]=public_tweets\n",
    "pickle_it(last_200_tweets,\"last_200_tweets.pkl\")\n",
    "del(last_200_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_in = open(\"last_200_tweets.pkl\",\"rb\")\n",
    "last_200_tweets = pickle.load(pkl_in)\n",
    "pkl_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimate Analysis => analysis_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_store = {}\n",
    "for idx0,fake_news in enumerate(last_200_tweets): #Loop through spewers of fake news(CNN,FOX, etc)\n",
    "    logger.info(\"Analyzing sentiment for {}\".format(fake_news))\n",
    "    analysis_store[fake_news] = []\n",
    "    for idx1, e in enumerate(last_200_tweets[fake_news]):\n",
    "        analysis = sentiment_analysis(last_200_tweets[fake_news][idx1]['full_text'])\n",
    "        tweet_dtime = last_200_tweets[fake_news][idx1]['created_at']\n",
    "        analysis['dtime']=tweet_dtime\n",
    "        analysis_store[fake_news].append(analysis)\n",
    "        if idx1 % 20 == 0:\n",
    "            logger.info(\"{} analysis is: {}\".format(fake_news,analysis))\n",
    "            logger.info(\"Analyzing tweet {} of {} from {}\".\\\n",
    "                        format(idx1,len(last_200_tweets[fake_news]),fake_news))\n",
    "        uname = e['user']['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Sentiment(save analysis_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis_store['CNN'][55] retrieves 55th tweet\n",
    "pickle_it(analysis_store,\"sentiment_analysis.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpickle to analysis_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_in = open(\"sentiment_analysis.pkl\",\"rb\")\n",
    "analysis_store = pickle.load(pkl_in)\n",
    "pkl_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_vectors = combine_vectors(analysis_store)\n",
    "#fake_news_vectors['CBS']['dtime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put Sentiment Analysis in DataFrame and Pickle(save df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = {}\n",
    "for fake_news_name in fake_news_vectors:\n",
    "    for type_ in type_handles:\n",
    "        df_data[fake_news_name+'_'+type_] = fake_news_vectors[fake_news_name][type_]\n",
    "df = pd.DataFrame(df_data)\n",
    "pickle_it(df,\"fake_news_sentiment_analysis_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpickle to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_in = open(\"fake_news_sentiment_analysis_df.pkl\",\"rb\")\n",
    "df = pickle.load(pkl_in)\n",
    "pkl_in.close()\n",
    "df = df.iloc[::-1] #flip table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Compound Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "xr = [i for i in range(-1*df.shape[0]+1,1)]\n",
    "df['tago']=xr\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(15,9)\n",
    "#df.plot.scatter(x='tago',y='BBC',c=['g'],s=50)\n",
    "#ax = df.plot(kind='scatter', x='tago', y='BBC', color='r')    \n",
    "#df.plot(kind='scatter', x='tago', y='CBS',color='g')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "sz=120\n",
    "sz2 = 100 #Requirements call for last 100 tweets\n",
    "ax.scatter(xr[100:], df['BBC_compound'].tolist()[100:], s=sz, c='b', marker=\"o\", label='BBC')\n",
    "ax.scatter(xr[100:],df['CBS_compound'].tolist()[100:],s=sz,c='y',marker='o',label='CBS')\n",
    "ax.scatter(xr[100:],df['Fox_compound'].tolist()[100:],s=sz,c='g',marker='o',label='Fox')\n",
    "ax.scatter(xr[100:],df['NYTimes_compound'].tolist()[100:],s=sz,c='c',marker='o',label='NYT')\n",
    "ax.scatter(xr[100:],df['CNN_compound'].tolist()[100:],s=sz,c='r',marker='o',label='CNN')\n",
    "plt.legend(loc='lower center',ncol=2);\n",
    "ax.legend(frameon=True, loc='lower center', ncol=5, fontsize=20)\n",
    "ax.set_title('Sentiment Analysis for Media Tweets')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('sentiment_analysis_scatter.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bar Chart (using Seaborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "bbc_mean = sum(df['BBC_compound'].tolist()[0:sz2])/sz2\n",
    "cnn_mean = sum(df['CNN_compound'].tolist()[0:sz2])/sz2\n",
    "fox_mean = sum(df['Fox_compound'].tolist()[0:sz2])/sz2\n",
    "nyt_mean = sum(df['NYTimes_compound'].tolist()[0:sz2])/sz2\n",
    "cbs_mean = sum(df['CBS_compound'].tolist()[0:sz2])/sz2\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(font_scale=2)  # kinda big\n",
    "bar_plot = sns.barplot(x=pd.Series(['BBC','CNN','Fox','NYT','CBS']),\\\n",
    "                       y=pd.Series([bbc_mean,cnn_mean,fox_mean,nyt_mean,cbs_mean]),\\\n",
    "                       palette = \"muted\")\n",
    "bar_plot.set(ylabel='Tweet Polarity',title='Overall Media Sentiment(Twitter)')\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Seaborn BarPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = bar_plot.get_figure()\n",
    "fig.savefig('sentiment_analysis_bar.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save DataFrame to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sentiment_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
