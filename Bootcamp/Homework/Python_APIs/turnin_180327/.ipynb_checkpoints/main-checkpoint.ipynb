{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python API's\n",
    "## John M. Warlop\n",
    "## UCSD Bootcamp Spring 2018\n",
    "## Due 3/31/18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import codecs\n",
    "import openweather as ow\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import time\n",
    "\n",
    "WAIT_TIME = 1.2\n",
    "SAMPLE_SIZE  = 20   #s/b 20 for release\n",
    "SAMPLE_SIZE2 = 500  #s/b 500 for release\n",
    "raw_fpath = '../raw_data/worldcitiespop.csv'\n",
    "pickle_fpath = '../raw_data/cities_sorted_by_lat.pkl'\n",
    "s_fname = 'api_keys'\n",
    "api_keys = {}\n",
    "with open(s_fname) as f:\n",
    "    api_keys = json.load(f)\n",
    "\n",
    "skip_load = True #Set false to reload raw file\n",
    "# Config Logger\n",
    "LOG_FORMAT = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "logging.basicConfig(filename=\"r_log.log\",level=logging.INFO,format=LOG_FORMAT,filemode='w')\n",
    "logger = logging.getLogger()\n",
    "# \n",
    "def file_kosher(reader):\n",
    "    fcount = 0\n",
    "    rcount = 0 \n",
    "    for row in reader:\n",
    "        if fcount == 0: #header row\n",
    "            fcount = len(row) #all rows should have this number of fields\n",
    "        if fcount == 0 or len(row) != fcount:\n",
    "            return(0)\n",
    "        rcount += 1\n",
    "    return(rcount)\n",
    "\n",
    "def lat_xover(lat_l):\n",
    "    c = 0\n",
    "    for e in lat_l:\n",
    "        if e[5] > 0:\n",
    "            return c\n",
    "        c += 1\n",
    "    return(0)\n",
    "\n",
    "def load_pickle(logger,ppath):\n",
    "    logger.info(\"Opening pickle file {}\".format)\n",
    "    pickle_in = open(pickle_fpath,\"rb\")\n",
    "    sorted_by_lat = pickle.load(pickle_in)\n",
    "    col = ['CC','City','City2','U','U2','Lat','Lon']\n",
    "    df = pd.DataFrame(sorted_by_lat,columns=col)\n",
    "    pickle_in.close()\n",
    "    logger.info(\"Pickle file loaded\")\n",
    "    return( (sorted_by_lat, df))\n",
    "\n",
    "def create_hemisphere_dataframe(df,degree_bins,dict_of_df,logger,hemisphere):\n",
    "    for idx, lat in enumerate(degree_bins): \n",
    "        if idx == len(degree_bins)-1:break \n",
    "        logger.debug(\"Degree bin({}) is: {} to {}\".\\\n",
    "                format(hemisphere,degree_bins[idx],degree_bins[idx+1]))\n",
    "        if hemisphere.find('outh') >= 1: #South\n",
    "            criteria1 = df['Lat'] <  degree_bins[idx]\n",
    "            criteria2 = df['Lat'] >= degree_bins[idx+1]\n",
    "        else: #North\n",
    "            criteria1 = df['Lat'] >  degree_bins[idx]\n",
    "            criteria2 = df['Lat'] <= degree_bins[idx+1]\n",
    "        dict_of_df[idx] = df[criteria1 & criteria2]\n",
    "        logger.debug(\"dict_of_df[{}] size of df is: {}\".\\\n",
    "                   format(idx,dict_of_df[idx].size))\n",
    "        \n",
    "def request_weather_and_populate_tups(api_keys,list_of_tuples,dict_of_df,logger,hemisphere):\n",
    "    logger.info(\"Fn: request_weather_and_populate_tups\")\n",
    "    logger.debug(\"dict_of_df size is: {}, sample size is: {}\".\\\n",
    "        format(len(dict_of_df),SAMPLE_SIZE))\n",
    "    for idx in range(len(dict_of_df)):\n",
    "        logger.debug(\"increment #{} of dict_of_df size {}\".format(idx,len(dict_of_df)))\n",
    "        logger.debug(\"dict_of_df[{}] df size is: {}\".format(idx,dict_of_df[idx].size))\n",
    "        df_25 = dict_of_df[idx].sample(SAMPLE_SIZE,replace=True)\n",
    "        logger.info(\"df_25 size is: {}\".format(df_25.size))\n",
    "        rcount = 0\n",
    "        for row in df_25.itertuples():\n",
    "            city,lat_,lon_ = getattr(row,'City'),getattr(row,'Lat'),getattr(row,'Lon')\n",
    "            url = \"http://api.openweathermap.org/data/2.5/weather?lat=\"+\\\n",
    "                str(lat_)+\"&lon=\"+str(lon_)+\"&appid=\"+api_keys['OpenWeather']\n",
    "            r = requests.get(url).json()\n",
    "            tmp,hum,cl,wsp,c_id = (r[\"main\"][\"temp\"]-273)*(9/5)+32.0,r[\"main\"][\"humidity\"],\\\n",
    "                                    r[\"clouds\"][\"all\"],r[\"wind\"][\"speed\"],r[\"id\"]\n",
    "            logger.debug(\"City {}: {}, {}, {}, {}, {}, {}, {}\".\\\n",
    "                    format(city,lat_,lon_,tmp,hum,cl,wsp,c_id))\n",
    "            logger.info(\"Processing city {} set {} of row {}\".format(city,idx,rcount))\n",
    "            logger.info(\"{}\".format(url))\n",
    "            time.sleep(WAIT_TIME)\n",
    "            tups.append((city, lat_, lon_,tmp,hum,cl,wsp,c_id))\n",
    "            rcount += 1\n",
    "    logger.info(\"{} hemisphere is complete\".format(hemisphere))\n",
    "\n",
    "\n",
    "def make_scatter_plots(logger):\n",
    "    titles = ['Lat(deg) vs Temp(F)','Lat(deg) vs % Humidity','Latitude(deg) vs Wind Speed',\\\n",
    "              'Latitude(deg) vs % Cloudiness','Lon(deg) vs Lat(deg)']\n",
    "    fnames = ['lat_temp.png','lat_humidity.png','lat_wind.png','lat_clouds.png','lon_lat.png']\n",
    "    xnames = ['lat','lat','lat','lat','lon']\n",
    "    ynames = ['temp','humidity','wind','clouds','lat']\n",
    "    for idx, name in enumerate(titles):\n",
    "        weather500_df.plot.scatter(x=xnames[idx],y=ynames[idx])\n",
    "        fig = plt.gcf()\n",
    "        plt.title(name)\n",
    "        plt.show()\n",
    "        fig.savefig(fnames[idx])\n",
    "        logger.debug(\"Fn: make_scatter_plots: {} written\".format(fnames[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "if not skip_load:\n",
    "    error = False\n",
    "    logger.info(\"Open pickle: {}\".format(raw_fpath))\n",
    "    with open(raw_fpath,\"r\",encoding=\"ISO-8859-1\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        nrows = file_kosher(reader)\n",
    "        if not nrows:\n",
    "            error = True\n",
    "        reader = f.seek(0) #put back @ head\n",
    "        c = 0\n",
    "        reader = csv.reader(f)\n",
    "        file_l = []\n",
    "        next(reader,None)\n",
    "        for row in reader:\n",
    "            row[5] = float(row[5])\n",
    "            row[6] = float(row[6])\n",
    "            trow = tuple(row)\n",
    "            file_l.append(trow)\n",
    "            c += 1\n",
    "        logger.debug(\"Raw file loaded, sorting list\")\n",
    "        ss = sorted(file_l,key = lambda x: (x[5],x[6]))\n",
    "        pckl_out = open(pickle_fpath,\"wb\")\n",
    "        logger.info(\"Dumping sorted list to pickle name: {}\".format(pickle_fpath))\n",
    "        pickle.dump(ss,pckl_out)\n",
    "        pckl_out.close()\n",
    "        del(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open pickled file(sorted by latitutde)\n",
    "(sorted_by_lat, df) = load_pickle(logger,pickle_fpath)\n",
    "xover_idx,min_lat,max_lat = lat_xover(sorted_by_lat),sorted_by_lat[0][5],sorted_by_lat[-1][5]\n",
    "logger.debug(\"xover idx: {}, min_lat: {}, max_lat: {} \".format(xover_idx,min_lat,max_lat))\n",
    "logger.debug(\"First record: {}\".format(sorted_by_lat[0]))\n",
    "logger.debug(\"Last record: {}\".format(sorted_by_lat[-1]))\n",
    "logger.debug(\"Length of sorted list: {}\".format(len(sorted_by_lat)))\n",
    "# Get Southern Hemisphere degree increments\n",
    "south_deg_bins = [i*abs(min_lat/20.0)*-1.0 for i in range(0,21)]\n",
    "logger.debug(\"South deg bin: {}\".format(south_deg_bins))\n",
    "logger.debug(\"South deg bin size: {}\".format(len(south_deg_bins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "south_df={}\n",
    "create_hemisphere_dataframe(df,south_deg_bins,south_df,logger,\"South\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tups = []\n",
    "request_weather_and_populate_tups(api_keys,tups,south_df,logger,\"Southern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Northern Hemisphere degree increments\n",
    "north_deg_bins = [i*abs(max_lat/20.0) for i in range(0,21)]\n",
    "logger.debug(\"North deg bin: {}\".format(north_deg_bins))\n",
    "logger.debug(\"North deg bin size: {}\".format(len(north_deg_bins)))        \n",
    "north_df={}\n",
    "create_hemisphere_dataframe(df,north_deg_bins,north_df,logger,\"North\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(\"north_df[0] size is: \".format(north_df[0].size))\n",
    "request_weather_and_populate_tups(api_keys,tups,north_df,logger,\"Northern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['City','lat','lon','temp','humidity','clouds','wind','c_id']\n",
    "weather_df = pd.DataFrame(tups,columns=cols)\n",
    "weather_df.to_pickle('../raw_data/weather_df.pkl')\n",
    "logger.info(\"End: Build Analysis Dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Begin: Plot & Save Images\")\n",
    "import arff\n",
    "weather_df = pd.read_pickle('../raw_data/weather_df.pkl')\n",
    "if weather_df.size >= SAMPLE_SIZE2:\n",
    "    weather500_df = weather_df.sample(SAMPLE_SIZE2,replace=True)\n",
    "    weather500_df.to_pickle('../raw_data/weather500_df.pkl')\n",
    "    weather500_df.to_csv(\"weather500_df.csv\")\n",
    "else:\n",
    "    logger.info(\"Unable to sample of size {}\".format(SAMPLE_SIZE2))\n",
    "#\n",
    "logger.debug(\"Size of weather500_df is: {}\".format(weather500_df.size))\n",
    "make_scatter_plots(logger)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean Log File\n",
    "import re\n",
    "with open(\"r_log.log\",\"r\") as f:\n",
    "    data = f.read()\n",
    "f.close()\n",
    "ndata = re.sub(r'appid=\\S+\\s','appid=**hidden** ',data)\n",
    "with open(\"r_log_clean.log\",\"w\") as f2:\n",
    "    f2.write(ndata)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
